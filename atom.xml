<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>XiaoQifeng&#39;s Blog</title>
  
  <subtitle>stay hungry, stay foolish</subtitle>
  <link href="https://alchemist1024.github.io/atom.xml" rel="self"/>
  
  <link href="https://alchemist1024.github.io/"/>
  <updated>2024-06-27T13:55:13.542Z</updated>
  <id>https://alchemist1024.github.io/</id>
  
  <author>
    <name>XiaoQifeng</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>工作四年啦</title>
    <link href="https://alchemist1024.github.io/2024/06/27/%E5%B7%A5%E4%BD%9C%E5%9B%9B%E5%B9%B4%E5%95%A6/"/>
    <id>https://alchemist1024.github.io/2024/06/27/%E5%B7%A5%E4%BD%9C%E5%9B%9B%E5%B9%B4%E5%95%A6/</id>
    <published>2024-06-27T14:05:20.000Z</published>
    <updated>2024-06-27T13:55:13.542Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2024/06/27/%E5%B7%A5%E4%BD%9C%E5%9B%9B%E5%B9%B4%E5%95%A6/2.jpg" alt="2"></p><center style="color:#C0C0C0;text-decoration:underline">某次出差回程的美景</center><p>时光匆匆，四年前走出校园开始了职业生涯，四年期间经历了3家公司，从一个职场小菜鸡变成了职场老菜鸡；最近经历了一些的事情所以记录一下，怕未来的某天忘记了这段经历。</p><a id="more"></a><p>2020年6月底，入职了第一家公司A，这里的工作更像是在校园上学，有宿舍有食堂，同事之间的相处也像老师同学之间的相处一般，在这里对自己职业发展有了很清晰的认识；后来因为个人原因离开了，现在想来这一段很愉快的经历。</p><p>2021年7月，入职了第二家公司B，入职的小组竟然遇到了以前一起实习的朋友，有时候世界真的很小；在这里的第一年被迫做了自己不喜欢的工作，遭遇成果被人摘走的经历；第二年终于想明白了自己想要什么，也争取到了自己喜欢的工作；也许是随性惯了，想出去看看所以离开了，在这里见识到了什么才是职场，何为人心。</p><p>2023年10月，入职了第三家公司C，满怀希望的来到这里，以为能学习到很多东西，实际却干拧螺丝的事，一度陷入了迷茫；在这里遇到了一件非常难忘的事“换领导”，入职3个月第一次换领导，过了1个月换领导，过了2个月左右换领导，过了3个月换领导，不知道自己什么时候变成了“天克领导圣体”，这也导致了身边一个又一个同事的离开，本就迷茫的自己现在更多了些无奈。</p><p>最近一直在想我在这里学到了什么，技能上好像什么也没学到甚至退化了，以前的自己时常给自己充充电，现在的自己晚上回到家只想睡觉，不漏电就不错了；看着收藏夹里越来越多的内容，想起了b友常说的两句话“到我收藏夹吃灰吧｜收藏从未停止，学习从未开始”，想到和做到之间真的有很大的鸿沟，也不知道从什么时候开始自己的拖延症这么严重了；虽然技能上没有什么成长，很幸运遇到了一些优秀且有趣的人，和有趣的人一起久了自然也会模仿他们身上好的习惯（例如读书，写文档)，可能这是在这里最大的收获吧。</p><p>自以为是一个内心强大的人，但是在这里工作仿佛在修行一般，千奇百怪的事不断地磨炼自己的心性；也罢，人生何处不修行。虽然很不开心，也许老天眷恋，遇到了令人欣慰的人和事；如果哪天离开了，也许真的没有值得学习和留恋的东西了。</p><p>流水账写完了，回想起来开始的时候最美好；希望以后自己少想多做，少拖延，及时行乐，迷茫的人多读书！！！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2024/06/27/%E5%B7%A5%E4%BD%9C%E5%9B%9B%E5%B9%B4%E5%95%A6/2.jpg&quot; alt=&quot;2&quot;&gt;&lt;/p&gt;
&lt;center style=&quot;color:#C0C0C0;text-decoration:underline&quot;&gt;某次出差回程的美景&lt;/center&gt;

&lt;p&gt;时光匆匆，四年前走出校园开始了职业生涯，四年期间经历了3家公司，从一个职场小菜鸡变成了职场老菜鸡；最近经历了一些的事情所以记录一下，怕未来的某天忘记了这段经历。&lt;/p&gt;</summary>
    
    
    
    <category term="杂记" scheme="https://alchemist1024.github.io/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
    <category term="生活" scheme="https://alchemist1024.github.io/tags/%E7%94%9F%E6%B4%BB/"/>
    
  </entry>
  
  <entry>
    <title>LLM面经整理</title>
    <link href="https://alchemist1024.github.io/2024/04/05/LLM%E9%9D%A2%E7%BB%8F/"/>
    <id>https://alchemist1024.github.io/2024/04/05/LLM%E9%9D%A2%E7%BB%8F/</id>
    <published>2024-04-05T14:55:12.000Z</published>
    <updated>2024-04-06T02:18:51.112Z</updated>
    
    <content type="html"><![CDATA[<p><strong>2023我遇到的八股</strong></p><ol><li><p>多头注意力有什么用？</p></li><li><p>position embedding的用法？</p></li><li><p>transformer, bert, gpt区别?</p></li><li><p>gpt推理时如何做的?</p><a id="more"></a></li><li><p>loss.backward()如果多次会出现什么情况呢?</p><p><a href="https://blog.51cto.com/u_15792804/5679035">https://blog.51cto.com/u_15792804/5679035</a></p><p><a href="https://blog.csdn.net/a845717607/article/details/104598278/">https://blog.csdn.net/a845717607/article/details/104598278/</a></p></li><li><p>llama2数据做了哪些处理?</p></li><li><p>大型模型训练到什么时候算好，如果基座模型生成不好的话，怎么去操作？1</p></li><li><p>生成的上下文关联不是很好如何调整?</p></li><li><p>MHA、MQA、CQA？</p></li><li><p>KV cache</p></li><li><p>如何缓解模型幻觉问题？</p></li><li><p>langchain组成？</p></li><li><p>llama(causal language model)、llama2(causal language model)、chatglm(casual with prefix)、chatglm2(causal language model)区别？</p></li><li><p>训练LLM需要多少数据？</p></li><li><p>混合精度训练？</p></li><li><p>RMSNorm和layerNorm区别？</p></li><li><p>layerNorm和batchNorm区别？</p></li><li><p>deepspeed的zero1、zero2、zero3介绍、数据并行？</p></li><li><p>LLM上下文扩展方法？</p></li><li><p>生成任务中的采样方法？</p></li><li><p>p-tuning和p-tuning-v2区别？</p></li><li><p>lora原理？为什么有效？</p></li><li><p>LLM高效训练方案？</p></li><li><p>BERT结构？预训练任务？</p></li><li><p>GLM结构？</p></li><li><p>什么是梯度累计？</p></li></ol><hr><p><strong>2024看看别人的八股(做个增量)</strong></p><ol start="27"><li><p>BERT训练时使用的学习率 warm-up 策略是怎样的？为什么要这么做</p></li><li><p>介绍一下 RAG? RAG 解决了哪些问题?</p></li><li><p>lora的矩阵怎么初始化？为什么要初始化为全0？</p></li><li><p>gpt源码past_key_value是干啥的？</p></li><li><p>encoder-only, decoder-only, encoder-decoder的区别？</p></li><li><p>介绍flash attention、flash attention-2?</p></li><li><p>什么是大模型外推性？</p></li><li><p>过拟合、欠拟合、如何缓解？</p></li><li><p>为什么说大模型训练很难？</p><p><a href="https://www.zhihu.com/question/498271491">https://www.zhihu.com/question/498271491</a></p></li><li><p>介绍一下 rouge，bleu，他们两个之间有什么区别？</p></li><li><p>RAG(上强度？)</p><ol><li>聊一下RAG项目总体思路？</li><li>使用外挂知识库主要是为了解决什么问题？</li><li>如何评价RAG项目的效果好坏，即指标是什么？</li><li>在做RAG项目过程中遇到哪些问题？怎么解决的？</li><li>RAG项目里面有哪一些亮点？目前开源的RAG项目非常多，你的项目和他们有什么区别？</li><li>数据集怎么构建的，什么规模，评估指标是什么，这些指标存在哪些问题？</li><li>模型底座是什么，这些不同底座什么区别，什么规模？</li><li>使用哪一种训练方法，什么sft，这些方法有什么不同，有什么优缺点，原理上解释不不同方法的差别？</li><li>模型推理是怎么做的，有没有cot，tot等等，还是单轮？</li><li>大模型可控性如何实现，怎么保证可控性？</li><li>模型部署的平台，推理效率怎么样，如何提升推理效率？</li><li>项目最后上线了么，上线之后发现什么问题，如何解决？</li><li>给一个总的输入输出样例，每一步包含什么prompt，多轮推理每一步输出什么结果，模拟一下，数据集格式是否要调整成这样，数据形式是什么，怎么拆分成多轮形式？</li></ol></li><li><p>大模型加速框架了解多少，知不知道原理 如何进行加速优化？</p><ol><li>vLLM<ul><li>vLLm 运行大模型非常快主要使用以下方法实现的</li><li>先进的服务吞吐量</li><li>通过PageAttention 对attention key &amp; value 内存进行有效的管理</li><li>对于输入请求的连续批处理</li><li>高度优化的CUDA kernels</li></ul></li><li>OpenLLM<ul><li>OpenLLM 运行大模型非常快主要使用以下方法实现的</li><li>促进实际生产过程中的大模型的部署，微调，服务和监测.</li></ul></li><li>DeepSpeed-MII<ul><li>DeepSpeed-MII 运行大模型非常快主要使用以下方法实现的</li><li>MII(Model Implementations for Inference) 提供加速的文本生成推理通过Blocked KV Caching, Continuous Batching, Dynamic SplitFuse 和高性能的CUDA Kernels</li></ul></li><li>TensorRT-llm<ul><li>DeepSpeed-MII 运行大模型非常快主要使用以下方法实现的</li><li>组装优化大语言模型推理解决方案的工具，提供Python API 来定义大模型，并为 NVIDIA GPU 编译高效的 TensorRT 引擎.</li></ul></li></ol></li><li><p>大模型中常见的位置编码？</p></li><li><p>分布式训练框架都了解哪些，能不能简单介绍一下?</p></li><li><p>pre normalization 和 post normalization？</p></li><li><p>几种主流大模型的 loss 了解过吗? 有哪些异同?</p></li><li><p>RLHF的具体工程是什么?包含了哪几个模型?</p></li><li><p>bert参数量的推演，任何一个transformer结构参数量的推演，和显存占用关系的推演。</p></li><li><p>bert self-attention中为什么要除根号d?</p></li><li><p>除了loss之外，如何在训练过程中监控模型能力？</p></li><li><p>如何评测生成，改写等开放性任务?</p></li><li><p>什么是大模型的复读机问题？如何缓解？</p></li><li><p>如何解决大模型落地过程中的bad case</p></li><li><p>如何缓解模型finetune后遗忘通用的能力</p></li><li><p>介绍scaling law</p><p><a href="https://zhuanlan.zhihu.com/p/667489780">https://zhuanlan.zhihu.com/p/667489780</a></p></li><li><p>Bert中为什么要在开头加个[CLS]?</p></li><li><p>tokenizer(BPE、word-piece、sentence-piece)</p></li><li><p>RHLF留个坑~</p></li><li><p>Bert为什么三个embedding可以相加？</p><p><a href="https://www.zhihu.com/question/374835153/answer/1080315948">https://www.zhihu.com/question/374835153/answer/1080315948</a></p></li></ol><p>先写这么多吧，yuanshen启动~</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;2023我遇到的八股&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;多头注意力有什么用？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;position embedding的用法？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;transformer, bert, gpt区别?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;gpt推理时如何做的?&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary>
    
    
    
    <category term="面试" scheme="https://alchemist1024.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
    <category term="NLP" scheme="https://alchemist1024.github.io/tags/NLP/"/>
    
    <category term="LLM" scheme="https://alchemist1024.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>开博语</title>
    <link href="https://alchemist1024.github.io/2020/07/15/%E5%BC%80%E5%8D%9A/"/>
    <id>https://alchemist1024.github.io/2020/07/15/%E5%BC%80%E5%8D%9A/</id>
    <published>2020-07-15T09:59:46.000Z</published>
    <updated>2024-04-05T14:54:54.945Z</updated>
    
    <content type="html"><![CDATA[<p>常言道“好记性不如烂笔头”，自认为不是一个聪明的人，所以开此博客记录学习笔记及生活趣事。</p><p>最最最重要的当然是多交朋友啦<code>(*^_^*)</code>。</p><p>2024.4.5来过，从2020年开了这个博客，一篇文章也没写，懒人一枚~</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;常言道“好记性不如烂笔头”，自认为不是一个聪明的人，所以开此博客记录学习笔记及生活趣事。&lt;/p&gt;
&lt;p&gt;最最最重要的当然是多交朋友啦&lt;code&gt;(*^_^*)&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;2024.4.5来过，从2020年开了这个博客，一篇文章也没写，懒人一枚~&lt;/p&gt;
</summary>
      
    
    
    
    <category term="杂记" scheme="https://alchemist1024.github.io/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
  </entry>
  
</feed>
