<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>LLM面经整理</title>
      <link href="2024/04/05/LLM%E9%9D%A2%E7%BB%8F/"/>
      <url>2024/04/05/LLM%E9%9D%A2%E7%BB%8F/</url>
      
        <content type="html"><![CDATA[<p><strong>2023我遇到的八股</strong></p><ol><li><p>多头注意力有什么用？</p></li><li><p>position embedding的用法？</p></li><li><p>transformer, bert, gpt区别?</p></li><li><p>gpt推理时如何做的?</p><a id="more"></a></li><li><p>loss.backward()如果多次会出现什么情况呢?</p><p><a href="https://blog.51cto.com/u_15792804/5679035">https://blog.51cto.com/u_15792804/5679035</a></p><p><a href="https://blog.csdn.net/a845717607/article/details/104598278/">https://blog.csdn.net/a845717607/article/details/104598278/</a></p></li><li><p>llama2数据做了哪些处理?</p></li><li><p>大型模型训练到什么时候算好，如果基座模型生成不好的话，怎么去操作？1</p></li><li><p>生成的上下文关联不是很好如何调整?</p></li><li><p>MHA、MQA、CQA？</p></li><li><p>KV cache</p></li><li><p>如何缓解模型幻觉问题？</p></li><li><p>langchain组成？</p></li><li><p>llama(causal language model)、llama2(causal language model)、chatglm(casual with prefix)、chatglm2(causal language model)区别？</p></li><li><p>训练LLM需要多少数据？</p></li><li><p>混合精度训练？</p></li><li><p>RMSNorm和layerNorm区别？</p></li><li><p>layerNorm和batchNorm区别？</p></li><li><p>deepspeed的zero1、zero2、zero3介绍、数据并行？</p></li><li><p>LLM上下文扩展方法？</p></li><li><p>生成任务中的采样方法？</p></li><li><p>p-tuning和p-tuning-v2区别？</p></li><li><p>lora原理？为什么有效？</p></li><li><p>LLM高效训练方案？</p></li><li><p>BERT结构？预训练任务？</p></li><li><p>GLM结构？</p></li><li><p>什么是梯度累计？</p></li></ol><hr><p><strong>2024看看别人的八股(做个增量)</strong></p><ol start="27"><li><p>BERT训练时使用的学习率 warm-up 策略是怎样的？为什么要这么做</p></li><li><p>介绍一下 RAG? RAG 解决了哪些问题?</p></li><li><p>lora的矩阵怎么初始化？为什么要初始化为全0？</p></li><li><p>gpt源码past_key_value是干啥的？</p></li><li><p>encoder-only, decoder-only, encoder-decoder的区别？</p></li><li><p>介绍flash attention、flash attention-2?</p></li><li><p>什么是大模型外推性？</p></li><li><p>过拟合、欠拟合、如何缓解？</p></li><li><p>为什么说大模型训练很难？</p><p><a href="https://www.zhihu.com/question/498271491">https://www.zhihu.com/question/498271491</a></p></li><li><p>介绍一下 rouge，bleu，他们两个之间有什么区别？</p></li><li><p>RAG(上强度？)</p><ol><li>聊一下RAG项目总体思路？</li><li>使用外挂知识库主要是为了解决什么问题？</li><li>如何评价RAG项目的效果好坏，即指标是什么？</li><li>在做RAG项目过程中遇到哪些问题？怎么解决的？</li><li>RAG项目里面有哪一些亮点？目前开源的RAG项目非常多，你的项目和他们有什么区别？</li><li>数据集怎么构建的，什么规模，评估指标是什么，这些指标存在哪些问题？</li><li>模型底座是什么，这些不同底座什么区别，什么规模？</li><li>使用哪一种训练方法，什么sft，这些方法有什么不同，有什么优缺点，原理上解释不不同方法的差别？</li><li>模型推理是怎么做的，有没有cot，tot等等，还是单轮？</li><li>大模型可控性如何实现，怎么保证可控性？</li><li>模型部署的平台，推理效率怎么样，如何提升推理效率？</li><li>项目最后上线了么，上线之后发现什么问题，如何解决？</li><li>给一个总的输入输出样例，每一步包含什么prompt，多轮推理每一步输出什么结果，模拟一下，数据集格式是否要调整成这样，数据形式是什么，怎么拆分成多轮形式？</li></ol></li><li><p>大模型加速框架了解多少，知不知道原理 如何进行加速优化？</p><ol><li>vLLM</li></ol><ul><li><p>vLLm 运行大模型非常快主要使用以下方法实现的</p></li><li><ul><li>先进的服务吞吐量</li><li>通过PageAttention 对attention key &amp; value 内存进行有效的管理</li><li>对于输入请求的连续批处理</li><li>高度优化的CUDA kernels</li></ul></li></ul><ol><li>OpenLLM</li></ol><ul><li><p>OpenLLM 运行大模型非常快主要使用以下方法实现的</p></li><li><ul><li>促进实际生产过程中的大模型的部署，微调，服务和监测.</li></ul></li></ul><ol><li>DeepSpeed-MII</li></ol><ul><li><p>DeepSpeed-MII 运行大模型非常快主要使用以下方法实现的</p></li><li><ul><li>MII(Model Implementations for Inference) 提供加速的文本生成推理通过Blocked KV Caching, Continuous Batching, Dynamic SplitFuse 和高性能的CUDA Kernels</li></ul></li></ul><ol><li>TensorRT-llm</li></ol><ul><li><p>DeepSpeed-MII 运行大模型非常快主要使用以下方法实现的</p></li><li><p>组装优化大语言模型推理解决方案的工具，提供Python API 来定义大模型，并为 NVIDIA GPU 编译高效的 TensorRT 引擎.</p></li></ul></li><li><p>大模型中常见的位置编码？</p></li><li><p>分布式训练框架都了解哪些，能不能简单介绍一下?</p></li><li><p>pre normalization 和 post normalization？</p></li><li><p>几种主流大模型的 loss 了解过吗? 有哪些异同?</p></li><li><p>RLHF的具体工程是什么?包含了哪几个模型?</p></li><li><p>bert参数量的推演，任何一个transformer结构参数量的推演，和显存占用关系的推演。</p></li><li><p>bert self-attention中为什么要除根号d?</p></li><li><p>除了loss之外，如何在训练过程中监控模型能力？</p></li><li><p>如何评测生成，改写等开放性任务?</p></li><li><p>什么是大模型的复读机问题？如何缓解？</p></li><li><p>如何解决大模型落地过程中的bad case</p></li><li><p>如何缓解模型finetune后遗忘通用的能力</p></li><li><p>介绍scaling law</p><p><a href="https://zhuanlan.zhihu.com/p/667489780">https://zhuanlan.zhihu.com/p/667489780</a></p></li><li><p>Bert中为什么要在开头加个[CLS]?</p></li><li><p>tokenizer(BPE、word-piece、sentence-piece)</p></li><li><p>RHLF留个坑~</p></li><li><p>Bert为什么三个embedding可以相加？</p><p><a href="https://www.zhihu.com/question/374835153/answer/1080315948">https://www.zhihu.com/question/374835153/answer/1080315948</a></p></li></ol><p>先写这么多吧，yuanshen启动，溜了~</p>]]></content>
      
      
      <categories>
          
          <category> 面试 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>开博语</title>
      <link href="2020/07/15/%E5%BC%80%E5%8D%9A/"/>
      <url>2020/07/15/%E5%BC%80%E5%8D%9A/</url>
      
        <content type="html"><![CDATA[<p>常言道“好记性不如烂笔头”，自认为不是一个聪明的人，所以开此博客记录学习笔记及生活趣事。</p><p>最最最重要的当然是多交朋友啦<code>(*^_^*)</code>。</p><p>2024.4.5来过，从2020年开了这个博客，一篇文章也没写，懒人一枚~</p>]]></content>
      
      
      <categories>
          
          <category> 杂记 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
